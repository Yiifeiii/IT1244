{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yiifeiii/IT1244/blob/main/Neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WYwmu9VBkXub",
        "outputId": "b87f9715-ea80-44f6-ac9b-d5c4e734af1c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Train_Set.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f8878fccfc15>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read the CSV file into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Train_Set.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Train_Set.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Train_Set.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "train_df = pd.read_csv('/content/Train_Set.csv')\n",
        "train_df_ = pd.read_csv('/content/Train_Set.csv')\n",
        "\n",
        "# TEST DATA\n",
        "test_df = pd.read_csv('/content/Test_Set.csv')\n",
        "test_df_ = pd.read_csv('/content/Test_Set.csv')\n",
        "\n",
        "# Display the DataFrame\n",
        "train_df.head()\n",
        "train_df.describe()\n",
        "train_df['class_label'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# General information about the dataset\n",
        "train_info = train_df.describe()\n",
        "test_info = test_df.describe()\n",
        "\n",
        "# Plot distribution of classes\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='class_label', data=train_df)\n",
        "plt.title('Distribution of Classes in Training Set')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot distribution of a few feature columns to understand their spread\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
        "for i, col in enumerate(['length_51', 'length_200', 'length_400']):\n",
        "    sns.histplot(train_df[col], ax=axes[i], kde=True)\n",
        "    axes[i].set_title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "train_info, test_info\n"
      ],
      "metadata": {
        "id": "QWKxkti9rdOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The class distribution in the training set shows that the dataset is class imbalanced, as seen in the distribution of classes.\n",
        "\n",
        "2. The features, represented by the lengths of DNA fragments (e.g., length_51, length_200, length_400), show varied distributions. Some have a right-skewed distribution, indicating a higher frequency of lower values.\n",
        "\n",
        "3. The general statistics for both training and test datasets indicate ranges of normalized frequencies for different lengths, with the means and standard deviations varying across different lengths. This variation suggests diversity in the DNA fragment lengths' normalized frequencies, which could be crucial for classifying the sample classes."
      ],
      "metadata": {
        "id": "xCNVSv3gsCWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "label healthy as 0 and cancer as 1 and Standardisation"
      ],
      "metadata": {
        "id": "wI41E0dlXalk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Recategorize the class_label in both datasets\n",
        "train_df['class_label'] = train_df['class_label'].apply(lambda x: 0 if x == 'healthy' else 1)\n",
        "test_df['class_label'] = test_df['class_label'].apply(lambda x: 0 if x == 'healthy' else 1)\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_df.drop('class_label', axis=1)\n",
        "y_train = train_df['class_label']\n",
        "X_test = test_df.drop('class_label', axis=1)\n",
        "y_test = test_df['class_label']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Perform PCA on the standardized train_set\n",
        "pca = PCA(n_components=10)  # Keep the first 10 principal components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "X_train_scaled.shape, X_train_pca.shape, y_train.value_counts(), y_test.value_counts()\n"
      ],
      "metadata": {
        "id": "GzcgXg4HfRRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the healthy and early stage cancer"
      ],
      "metadata": {
        "id": "80Wvgg09XmFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new dataset with only \"healthy\" and \"early stage\" cancer samples\n",
        "train_df_subset = train_df_[(train_df_['class_label'] == 'healthy') | (train_df_['class_label'] == 'early stage cancer')]\n",
        "test_df_subset = test_df_[(test_df_['class_label'] == 'healthy') | (test_df_['class_label'] == 'early stage cancer')]\n",
        "\n",
        "# Recategorize the class_label in both datasets\n",
        "train_df_subset['class_label'] = train_df_subset['class_label'].apply(lambda x: 0 if x == 'healthy' else 1)\n",
        "test_df_subset['class_label'] = test_df_subset['class_label'].apply(lambda x: 0 if x == 'healthy' else 1)\n",
        "\n",
        "# Separate features and labels\n",
        "X_train_subset = train_df_subset.drop('class_label', axis=1)\n",
        "y_train_subset = train_df_subset['class_label']\n",
        "X_test_subset = test_df_subset.drop('class_label', axis=1)\n",
        "y_test_subset = test_df_subset['class_label']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_subset_scaled = scaler.fit_transform(X_train_subset)\n",
        "X_test_subset_scaled = scaler.transform(X_test_subset)\n",
        "\n",
        "y_train_subset.value_counts(), y_test_subset.value_counts()\n"
      ],
      "metadata": {
        "id": "4rlkdiy7gT0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA for feature selection for cancer VS healthy"
      ],
      "metadata": {
        "id": "EE-OimWxY1aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA\n",
        "pca = PCA(n_components=10)  # You can choose the number of components as needed\n",
        "\n",
        "# Perform PCA on training data\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_train_pca_df = pd.DataFrame(data=X_train_pca, columns=[f'PC{i+1}' for i in range(10)])\n",
        "# Transform test data using the same PCA transformation\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "X_test_pca_df = pd.DataFrame(data=X_test_pca, columns=[f'PC{i+1}' for i in range(10)])\n",
        "\n",
        "X_train_pca_df.head(), pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "7MnjryR6Y3cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA for feature selection for healthy VS early stage cancer  "
      ],
      "metadata": {
        "id": "cnUKKtktZ-Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform PCA on early stage trainig data\n",
        "X_train_pca_early = pca.fit_transform(X_train_subset_scaled)\n",
        "X_train_pca_early_df = pd.DataFrame(data=X_train_pca_early, columns=[f'PC{i+1}' for i in range(10)])\n",
        "# Transform early stage test data using the same PCA transformation\n",
        "X_test_pca_early = pca.fit_transform(X_test_subset_scaled)\n",
        "X_test_pca_early_df = pd.DataFrame(data=X_test_pca_early, columns=[f'PC{i+1}' for i in range(10)])\n",
        "\n",
        "X_train_pca_early_df.head(), pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "Xzd1WqBZ2qkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train healthy VS cancer with multilayer neural network"
      ],
      "metadata": {
        "id": "jul5bpi5KIoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize and train the basic multilayer neural network model\n",
        "#mlp = MLPClassifier(hidden_layer_sizes=(1,), max_iter=1000, random_state=42)\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(1000,), max_iter=1000, random_state=42)\n",
        "mlp.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on the transformed test set (after applying the same standardization and PCA transformation)\n",
        "y_pred = mlp.predict(X_test_pca)\n",
        "y_pred_proba = mlp.predict_proba(X_test_pca)[:, 1]  # Probability estimates for AUROC\n",
        "\n",
        "# Calculate precision, recall, F1 score, and AUROC\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auroc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "precision, recall, f1, auroc"
      ],
      "metadata": {
        "id": "nZMjJTwiyMnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "precision: 0.96\n",
        "recall: 0.99\n",
        "f1: 0.97\n",
        "auroc: 0.81"
      ],
      "metadata": {
        "id": "eb8eCtEHpGhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train healthy VS early stage cancer with multilayer neural network"
      ],
      "metadata": {
        "id": "qC1waeZ0l7I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize and train the basic multilayer neural network model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "mlp.fit(X_train_pca_early, y_train_subset)\n",
        "\n",
        "# Predict on the transformed test set (after applying the same standardization and PCA transformation)\n",
        "y_pred_early = mlp.predict(X_test_pca_early)\n",
        "y_pred_proba_early = mlp.predict_proba(X_test_pca_early)[:, 1]  # Probability estimates for AUROC\n",
        "\n",
        "# Calculate precision, recall, F1 score, and AUROC\n",
        "precision = precision_score(y_test_subset, y_pred_early)\n",
        "recall = recall_score(y_test_subset, y_pred_early)\n",
        "f1 = f1_score(y_test_subset, y_pred_early)\n",
        "auroc = roc_auc_score(y_test_subset, y_pred_proba_early)\n",
        "\n",
        "precision, recall, f1, auroc"
      ],
      "metadata": {
        "id": "1zLt_NkUl6Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "precision: 0.90\n",
        "recall: 0.70\n",
        "f1: 0.79\n",
        "auroc: 0.56"
      ],
      "metadata": {
        "id": "ln0A8R6xpSWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "add error function to tackle imbalanced data"
      ],
      "metadata": {
        "id": "_OLyb1eIpXd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Define the MLP model with hyperbolic tangent activation function and a learning rate of 0.1\n",
        "\n",
        "# Define a range of hidden layer sizes to test (for simplicity, we vary the number of layers,\n",
        "# each with a fixed size of 100 neurons)\n",
        "hidden_layer_sizes = [(100,), (100, 100), (100, 100, 100)]  # Example: 1 layer, 2 layers, 3 layers\n",
        "\n",
        "# Setup grid search parameters\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': hidden_layer_sizes,\n",
        "}\n",
        "\n",
        "# Define MLPClassifier with fixed parameters\n",
        "mlp2 = MLPClassifier(activation='tanh', solver='adam', learning_rate_init=0.1, max_iter=1000, random_state=42)\n",
        "\n",
        "# Define the cross-validation scheme and grid search\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(estimator=mlp2, param_grid=param_grid, cv=cv, scoring=make_scorer(f1_score), n_jobs=-1)\n",
        "\n",
        "# Fit grid search (this can be time-consuming)\n",
        "grid_search.fit(X_train_pca, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "best_params, best_score\n"
      ],
      "metadata": {
        "id": "e09mBg7an9mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score: 0.99\n"
      ],
      "metadata": {
        "id": "Z3euqe2KKn-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Define the MLP model with hyperbolic tangent activation function and a learning rate of 0.1\n",
        "\n",
        "# Define a range of hidden layer sizes to test (for simplicity, we vary the number of layers,\n",
        "# each with a fixed size of 100 neurons)\n",
        "hidden_layer_sizes = [(100,), (100, 100), (100, 100, 100)]  # Example: 1 layer, 2 layers, 3 layers\n",
        "\n",
        "# Setup grid search parameters\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': hidden_layer_sizes,\n",
        "}\n",
        "\n",
        "# Define MLPClassifier with fixed parameters\n",
        "mlp2 = MLPClassifier(activation='tanh', solver='adam', learning_rate_init=0.1, max_iter=1000, random_state=42)\n",
        "\n",
        "# Define the cross-validation scheme and grid search\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(estimator=mlp2, param_grid=param_grid, cv=cv, scoring=make_scorer(f1_score), n_jobs=-1)\n",
        "\n",
        "# Fit grid search (this can be time-consuming)\n",
        "grid_search.fit(X_train_pca_early, y_train_subset)\n",
        "\n",
        "# Best parameters and score\n",
        "best_params_early = grid_search.best_params_\n",
        "best_score_early = grid_search.best_score_\n",
        "\n",
        "best_params_early, best_score_early\n"
      ],
      "metadata": {
        "id": "AszFB9xa1abu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score: 0.97"
      ],
      "metadata": {
        "id": "_mzWY7rLKsuR"
      }
    }
  ]
}